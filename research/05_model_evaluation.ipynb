{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Niranjan kumar\\\\mldemo\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Niranjan kumar\\\\mldemo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path : Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    all_params:dict\n",
    "    metric_file_name: Path\n",
    "    target_column : dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories, save_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.Logistic_Regression\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            test_data_path= config.test_data_path,\n",
    "            model_path= config.model_path,\n",
    "            all_params = params,\n",
    "            metric_file_name = config.metric_file_name,\n",
    "            target_column = schema,\n",
    "       \n",
    "        \n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config = ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def eval_metrics(self,actual, pred):\n",
    "        accuracy = accuracy_score(actual,pred)\n",
    "        clas_report = classification_report(actual,pred)\n",
    "        conf_matrix= confusion_matrix(actual,pred)\n",
    "        return accuracy, clas_report, conf_matrix\n",
    "    \n",
    "    def save_results(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        test_data = pd.read_csv(self.config.test_data_path, usecols=lambda column: column != 'id')\n",
    "        model = joblib.load(self.config.model_path)\n",
    "        \n",
    "        target_column_schema =list(self.config.target_column.keys())[0]\n",
    "    \n",
    "        label_encoder = LabelEncoder()\n",
    "        data[target_column_schema] = label_encoder.fit_transform(data[target_column_schema])\n",
    "        test_data[target_column_schema] = label_encoder.transform(test_data[target_column_schema])\n",
    "\n",
    "        data[target_column_schema]\n",
    "    \n",
    "        test_x = test_data.drop(columns=[target_column_schema],axis =1)\n",
    "\n",
    "        test_y = test_data[target_column_schema]\n",
    "    \n",
    "        predicted_qualities = model.predict(test_x)\n",
    "        \n",
    "        (accuracy, clas_report, conf_matrix) = self.eval_metrics(test_y, predicted_qualities)\n",
    "        conf_matrix = conf_matrix.tolist()\n",
    "        \n",
    "    # Saving ,metrics as local\n",
    "        scores = {'Accuracy': accuracy, 'clas_report': clas_report, 'conf_matrix': conf_matrix}\n",
    "        save_json(path= Path(self.config.metric_file_name),data = scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-11 00:38:44,010: INFO: common: yaml file: C:\\Users\\Niranjan kumar\\mldemo\\config\\config.yaml loaded successfully]\n",
      "[2024-03-11 00:38:44,013: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-11 00:38:44,020: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-03-11 00:38:44,023: INFO: common: created directory at: artifacts]\n",
      "[2024-03-11 00:38:44,025: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "0          12.21         14.09           78.78     462.00             0.08   \n",
      "1          13.66         15.15           88.27     580.60             0.08   \n",
      "2          12.83         15.73           82.89     506.90             0.09   \n",
      "3          11.06         17.12           71.25     366.50             0.12   \n",
      "4          19.19         15.94          126.30    1157.00             0.09   \n",
      "..           ...           ...             ...        ...              ...   \n",
      "138        13.05         18.59           85.09     512.00             0.11   \n",
      "139        11.30         18.19           73.93     389.40             0.10   \n",
      "140        13.77         22.29           90.63     588.90             0.12   \n",
      "141         9.67         18.10           61.06     286.30             0.08   \n",
      "142        14.95         18.77           97.84     689.50             0.08   \n",
      "\n",
      "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "0                0.08            0.07                 0.03           0.16   \n",
      "1                0.08            0.04                 0.02           0.18   \n",
      "2                0.08            0.06                 0.03           0.17   \n",
      "3                0.11            0.04                 0.04           0.20   \n",
      "4                0.12            0.12                 0.10           0.17   \n",
      "..                ...             ...                  ...            ...   \n",
      "138              0.13            0.10                 0.06           0.20   \n",
      "139              0.13            0.15                 0.03           0.21   \n",
      "140              0.13            0.14                 0.07           0.18   \n",
      "141              0.05            0.01                 0.01           0.17   \n",
      "142              0.12            0.09                 0.04           0.17   \n",
      "\n",
      "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
      "0                      0.06  ...         13.13          19.29   \n",
      "1                      0.06  ...         14.54          19.64   \n",
      "2                      0.06  ...         14.09          19.35   \n",
      "3                      0.08  ...         11.69          20.74   \n",
      "4                      0.05  ...         22.03          17.81   \n",
      "..                      ...  ...           ...            ...   \n",
      "138                    0.07  ...         14.19          24.85   \n",
      "139                    0.08  ...         12.58          27.96   \n",
      "140                    0.07  ...         16.39          34.01   \n",
      "141                    0.06  ...         11.15          24.62   \n",
      "142                    0.06  ...         16.25          25.47   \n",
      "\n",
      "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "0              87.65      529.90              0.10               0.24   \n",
      "1              97.96      657.00              0.13               0.31   \n",
      "2              93.22      605.80              0.13               0.26   \n",
      "3              76.08      411.10              0.17               0.20   \n",
      "4             146.60     1495.00              0.11               0.20   \n",
      "..               ...         ...               ...                ...   \n",
      "138            94.22      591.20              0.13               0.27   \n",
      "139            87.16      472.90              0.13               0.48   \n",
      "140           111.60      806.90              0.17               0.31   \n",
      "141            71.11      380.20              0.14               0.13   \n",
      "142           107.10      809.70              0.10               0.25   \n",
      "\n",
      "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0               0.31                  0.09            0.27   \n",
      "1               0.26                  0.11            0.34   \n",
      "2               0.35                  0.10            0.30   \n",
      "3               0.13                  0.10            0.28   \n",
      "4               0.23                  0.18            0.24   \n",
      "..               ...                   ...             ...   \n",
      "138             0.26                  0.13            0.31   \n",
      "139             0.74                  0.12            0.33   \n",
      "140             0.38                  0.17            0.31   \n",
      "141             0.06                  0.03            0.31   \n",
      "142             0.25                  0.08            0.29   \n",
      "\n",
      "     fractal_dimension_worst  \n",
      "0                       0.09  \n",
      "1                       0.10  \n",
      "2                       0.08  \n",
      "3                       0.12  \n",
      "4                       0.06  \n",
      "..                       ...  \n",
      "138                     0.08  \n",
      "139                     0.13  \n",
      "140                     0.09  \n",
      "141                     0.08  \n",
      "142                     0.09  \n",
      "\n",
      "[143 rows x 30 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      1\n",
      "      ..\n",
      "138    0\n",
      "139    0\n",
      "140    1\n",
      "141    0\n",
      "142    0\n",
      "Name: diagnosis, Length: 143, dtype: int32\n",
      "[2024-03-11 00:38:44,129: INFO: common: json file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.save_results()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
